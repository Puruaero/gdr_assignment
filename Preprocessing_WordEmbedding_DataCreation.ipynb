{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from html.parser import HTMLParser\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enchant import Dict\n",
    "dictionary = Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "h = HTMLParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_and_stem(word) :\n",
    "    \"\"\"\n",
    "    Function to lemmatization and stemming of a word based on dictionary checks\n",
    "    \"\"\"\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "    if lemmatized != word : \n",
    "        if dictionary.check(lemmatized)==True : \n",
    "            return lemmatized\n",
    "        else : \n",
    "            return porter.stem(word)\n",
    "    else : \n",
    "        stemmed = porter.stem(word)\n",
    "        if dictionary.check(stemmed) == True : \n",
    "            return stemmed\n",
    "        else : \n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, broken_sentences=False) : \n",
    "    \"\"\"\n",
    "    Function to clean sentences. If broken_sentences=True break the paragraphs in text to get the single sentences\n",
    "    in cleaned text\n",
    "\n",
    "    Steps Done are\n",
    "    1. html unescape \n",
    "    2. Remove Punctuations\n",
    "    3. Lemmatization and Stemming\n",
    "    4. Remove StopWords \n",
    "    \n",
    "    \"\"\"\n",
    "    cleaned_text = [] \n",
    "    if type(text) != list : \n",
    "        text = [text] \n",
    "    for paragraph in text :  \n",
    "        sentence_tokenized = sent_tokenize(paragraph)\n",
    "        cleaned_sentences = [] \n",
    "        for t in sentence_tokenized : \n",
    "            html_escaped_chars = h.unescape(t)\n",
    "            remove_punctuations = \"\".join([c for c in html_escaped_chars if not c in string.punctuation])\n",
    "            words = remove_punctuations.split(\" \")\n",
    "            lemmatized_and_stemmed = [lemmatize_and_stem(word) for word in words if len(word)>0]\n",
    "            stopwords_removed_words = [word for word in lemmatized_and_stemmed if not word in stop_words]\n",
    "            final_sentence = \" \".join(stopwords_removed_words)\n",
    "            cleaned_sentences.append(final_sentence)\n",
    "        if not broken_sentences : \n",
    "            cleaned_paragraph = \". \".join(cleaned_sentences)\n",
    "            cleaned_text.append(cleaned_paragraph)\n",
    "        else : \n",
    "            cleaned_text = cleaned_text + cleaned_sentences\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "data_dir = os.path.join(os.getcwd(), 'glassdoor_problem/data/')\n",
    "unlabelled_data = pd.read_pickle(os.path.join(data_dir, 'unlabelled_data.pkl'))\n",
    "labelled_data = pd.read_pickle(os.path.join(data_dir, 'labelled_data.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n"
     ]
    }
   ],
   "source": [
    "# clean all the sentences from the entire data\n",
    "all_lines = unlabelled_data.pros.tolist() + unlabelled_data.cons.tolist() + labelled_data.pp_sent.tolist()\n",
    "_ = random.shuffle(all_lines)\n",
    "cleaned_sentences = clean_text(all_lines, broken_sentences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions helpful to get the word2vec model and build the model and save it for using while training\n",
    "def tokenize_sentences_for_model(sentence) : \n",
    "    return word_tokenize(sentence)\n",
    "\n",
    "def form_word2vec_model(all_sentences, tokenized, window=5, min_count=1, workers=4, sg=1) : \n",
    "    \"\"\"\n",
    "    Given \n",
    "    all_sentences : List of sentences\n",
    "    tokenized : True or False, if False the word tokenization of sentences will be done within the function otherwise not\n",
    "    \"\"\"\n",
    "    if tokenized : \n",
    "        model = Word2Vec(all_sentences, window=window, min_count=min_count, workers=workers, sg=sg)\n",
    "    else : \n",
    "        all_sentences = [tokenize_sentences_for_model(sentence) for sentence in all_sentences]\n",
    "        model = Word2Vec(all_sentences, window=window, min_count=min_count, workers=workers, sg=sg)\n",
    "    return model\n",
    "\n",
    "wordvec_model = form_word2vec_model(cleaned_sentences, tokenized=False)\n",
    "_ = wordvec_model.save(os.path.join(os.getcwd(), 'glassdoor_problem/wordvecmodel'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Everything below from here to is to create the multilabel data to be used in training from given labelled data. \n",
    "\n",
    "NOTE(A STRATEGY): THE LABEL VECTORS FOR EACH SENTENCE IS SCALED 8 TIMES THE ORIGINAL LABEL\n",
    "\n",
    "The training/validation/test data created would look like [(sentence1, multilabel_label1), (sentence2, multilabel_label2)....]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'salary_benefits': 0,\n",
       " 'wlb_working_conditions': 1,\n",
       " 'tech_product': 2,\n",
       " 'culture_team': 3,\n",
       " 'Job Security/Advancement': 4,\n",
       " 'haras_discrim_sexism': 5,\n",
       " 'management': 6,\n",
       " 'business_vision_competitors': 7}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n"
     ]
    }
   ],
   "source": [
    "# PROCESS ONE LABEL DATA\n",
    "one_label_data = labelled_data[labelled_data.label.apply(lambda x: len(x)==1)]\n",
    "one_label_data.label = one_label_data.label.apply(lambda x: x[0])\n",
    "all_labels = one_label_data.label.value_counts().index.tolist()\n",
    "\n",
    "# An index for each label. This will be used later\n",
    "label_map = {}\n",
    "for i in range(len(all_labels)) : \n",
    "    label = all_labels[i]\n",
    "    label_map[label] = i\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'glassdoor_problem/label_map.pickle'), 'wb') as f: \n",
    "    pickle.dump(label_map, f)\n",
    "\n",
    "#A dictionary with one label sentences for fetching it directly while creating training data\n",
    "basic_preprocess_labeldata = {} \n",
    "for label in all_labels : \n",
    "    basic_preprocess_labeldata[label] = clean_text(one_label_data[one_label_data.label==label].pp_sent.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESS MULTILABEL DATA\n",
    "multilabel_data = labelled_data[labelled_data.label.apply(lambda x: len(x)>1)]\n",
    "multilabel_data.label = multilabel_data.label.apply(lambda x: tuple(x))\n",
    "multilabel_data = multilabel_data.drop_duplicates()\n",
    "multilabel_data.label = multilabel_data.label.apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n"
     ]
    }
   ],
   "source": [
    "# Create Training Data from the processed unilabel and multilabel sentences above\n",
    "# 500 sentences have been chosen for both validation and test, to keep training set around 80% of entire data\n",
    "def create_training_data_multilabel() :     \n",
    "    entire_training_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    # unilabel data\n",
    "    unilabel_data = [] \n",
    "    for label in basic_preprocess_labeldata : \n",
    "        label_index = label_map[label]\n",
    "        given_label_sentences = basic_preprocess_labeldata[label]\n",
    "        given_label_sentences = list(set(given_label_sentences))\n",
    "        ############################EIGHT TIMES SCALING BELOW###########################\n",
    "        train_label = [(sen, 8*np.eye(8)[label_index]) for sen in given_label_sentences]\n",
    "        unilabel_data = unilabel_data + train_label\n",
    "    \n",
    "    _ = random.shuffle(unilabel_data)\n",
    "    test_unilabel = unilabel_data[-500:]\n",
    "    validation_unilabel = unilabel_data[-1000:-500]\n",
    "    train_unilabel = unilabel_data[0:-1000]\n",
    "    \n",
    "    #multilabel data\n",
    "    multilabeldata = []\n",
    "    cleaned_multilable_messages = clean_text(multilabel_data.pp_sent.tolist())\n",
    "    for i in range(multilabel_data.shape[0]) :         \n",
    "        message = cleaned_multilable_messages[i]\n",
    "        labels = multilabel_data.label.iloc[i]\n",
    "        resulting_classification_vec = np.zeros(8)[0]\n",
    "        for label in labels : \n",
    "            label_index = label_map[label]\n",
    "            ############################EIGHT TIMES SCALING BELOW###########################\n",
    "            resulting_classification_vec = resulting_classification_vec + 8*np.eye(8)[label_index]\n",
    "            multilabeldata.append((message, resulting_classification_vec))\n",
    "            \n",
    "    _ = random.shuffle(multilabeldata)\n",
    "    test_multilabel = multilabeldata[-30:]\n",
    "    validation_multilabel = multilabeldata[-60:-30]\n",
    "    train_multilabel = multilabeldata[0:-60]\n",
    "    \n",
    "    all_train_data = train_unilabel + train_multilabel\n",
    "    _ = random.shuffle(all_train_data)\n",
    "    \n",
    "    all_validation_data = validation_unilabel + validation_multilabel\n",
    "    _ = random.shuffle(all_validation_data)\n",
    "    \n",
    "    all_test_data = test_unilabel + test_multilabel\n",
    "    _  = random.shuffle(all_test_data)\n",
    "    \n",
    "    return all_train_data, all_validation_data, all_test_data\n",
    "\n",
    "trd, vad, ted = create_training_data_multilabel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'glassdoor_problem')\n",
    "with open(os.path.join(data_dir, 'training_data.pkl'), 'wb') as f: \n",
    "    pickle.dump(trd, f)\n",
    "\n",
    "with open(os.path.join(data_dir, 'validation_data.pkl'), 'wb') as f: \n",
    "    pickle.dump(vad, f)\n",
    "    \n",
    "with open(os.path.join(data_dir, 'test_data.pkl'), 'wb') as f: \n",
    "    pickle.dump(ted, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('benefit decent compared job market', array([0., 0., 0., 0., 0., 0., 0., 8.]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "trd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
